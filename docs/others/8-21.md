命令更新（与你现有流程一致，只是路径更清晰）

导出数据集（保持你原脚本不变，仅换输出目录）

python scripts\export_yolo_dataset.py `
  --dataset-id $datasetId `
  --out-dir .\datasets\w_lid_ds3 `
  --val-ratio 0.15 `
  --test-ratio 0.05 `
  --api-base http://127.0.0.1:8000


训练

python scripts\train_yolo.py `
  --data .\datasets\w_lid_ds3\data.yaml `
  --model yolov8n.pt `
  --imgsz 640 `
  --epochs 50 `
  --batch 8 `
  --device 0 `
  --name w_lid_v1


测试（predict 可视化到 runs\detect\predict）

python scripts\predict_visualize.py `
  --weights .\runs\detect\w_lid_v1\weights\best.pt `
  --source .\datasets\w_lid_ds3\images\test `
  --project .\runs\detect `
  --name predict


验证（val）

yolo val model="runs\detect\w_lid_v1\weights\best.pt" `
         data="datasets\w_lid_ds3\data.yaml" `
         split=test device=0


标注 Lint（保持原脚本，按新路径）

python scripts\lint_yolo_labels.py `
  --images .\datasets\w_lid_ds3\images\train `
  --labels .\datasets\w_lid_ds3\labels\train `
  --classes "plate holder,1d barcode,2d barcode"


离线位姿 Demo（新增脚本的位置）

# 先把 camera.yaml / frames.yaml / grasp.yaml 放进 configs/
python scripts\offline_pose_demo.py `
  --image ".\datasets\w_lid_ds3\images\test\21791753181528_.pic.jpg" `
  --camera-config .\configs\camera.yaml `
  --frames-config .\configs\frames.yaml `
  --grasp-config .\configs\grasp.yaml `
  --yolo-weights .\runs\detect\w_lid_v1\weights\best.pt

README.md（精简版替换）

把下面内容直接覆盖你的 README.md（可再补充截图/指标）：

项目概览

services/：标注/相机等后端服务

scripts/：数据导出、训练、预测、校验、离线位姿

robot/：机器人侧可复用库（感知/规划/适配/任务编排）

datasets/：导出的 YOLO 数据集

runs/：Ultralytics 训练/预测输出

configs/：相机、坐标与抓取参数

安装
conda activate ann_backend
pip install -r requirements.txt
# Windows: 需要 OpenCV-contrib
pip install opencv-contrib-python ultralytics pyyaml numpy
# 让 Python 找到本仓库
# PowerShell
$env:PYTHONPATH = (Get-Location).Path

标注 → 导出 → 训练
# 导入图片、导入 YOLO txt（同你现有流程）
python scripts\import_yolo_annotations.py ...

# 导出数据集
python scripts\export_yolo_dataset.py --dataset-id <id> --out-dir .\datasets\w_lid_ds3

# 训练
python scripts\train_yolo.py --data .\datasets\w_lid_ds3\data.yaml --model yolov8n.pt --epochs 50 --imgsz 640 --batch 8 --device 0 --name w_lid_v1

评估/预测
yolo val model="runs\detect\w_lid_v1\weights\best.pt" data="datasets\w_lid_ds3\data.yaml" split=test device=0
python scripts\predict_visualize.py --weights runs\detect\w_lid_v1\weights\best.pt --source datasets\w_lid_ds3\images\test

标签 Lint
python scripts\lint_yolo_labels.py --images datasets\w_lid_ds3\images\train --labels datasets\w_lid_ds3\labels\train --classes "plate holder,1d barcode,2d barcode"

离线位姿（固定相机）
python scripts\offline_pose_demo.py --image <img> --camera-config configs\camera.yaml --frames-config configs\frames.yaml --grasp-config configs\grasp.yaml --yolo-weights runs\detect\w_lid_v1\weights\best.pt

说明与注意

不改你现有脚本的参数名/行为，只是把输出目录统一到 datasets/；训练/验证命令相应换路径即可。

机器人侧代码完全独立成包在 robot/，脚本只需 import robot...（已通过 PYTHONPATH 解决）。

pics/ 很大且在用，先别挪；后续再逐步搬到 data/raw/。

path\to\dir\detect 是 Ultralytics 某次默认写出的占位，可删。

grab_frams.py、requirments.txt 建议按上面更名，避免误会。

需要我把 configs/*.yaml 参考模板 和 robot/*.py 具体实现（带完整 Args/Returns/Raises docstring）一并发你吗？你点头我就贴全套文件。